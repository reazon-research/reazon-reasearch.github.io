<!doctype html>
<html class="no-js"  lang="ja" >

<head><meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-S1KMDX1V1H"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-S1KMDX1V1H');
    </script>
    <link rel="canonical" href="https://developers.agirobots.com/jp/how-to-run-aloha/" />
<meta name="robots" content="noindex, follow" />
  <link rel="author" title="このドキュメントについて" href="../about.html" /><link rel="index" title="索引" href="../genindex.html" /><link rel="search" title="検索" href="../search.html" /><link rel="next" title="(2024-02-14) ReazonSpeech v2.0: 音声モデルの高速化とコーパスの大幅な拡大" href="2024-02-14-ReazonSpeech.html" /><link rel="prev" title="(2024-08-01) ReazonSpeech v2.1: Setting a New Standard in Japanese ASR" href="2024-08-01-ReazonSpeech.html" />
  
  <title>(2024-03-02) ALOHAとACTを用いた模倣学習実験の再現方法 - Reazon Human Interaction Lab</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Roboto+Condensed:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
  <link rel="stylesheet" type="text/css" href="../_static/style.css" />
  <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
  </head>

<body id="blog-2024-03-02-how-to-run-aloha-developers.agirobots.com">
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
    <symbol id="svg-arrow" viewBox="0 0 14 8">
      <svg viewBox="0 0 14 8" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7 2.82801L2.05 7.77801L0.635998 6.36401L7 1.47024e-05L13.364 6.36402L11.95 7.77802L7 2.82801Z"
          fill="currentColor" />
      </svg>
    </symbol>
    <symbol id="svg-toc-menu">
      <svg width="19" height="17" viewBox="0 0 19 17" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path
          d="M19 15V17H1V15H19ZM4.596 0.903999L6.01 2.318L2.828 5.5L6.01 8.682L4.596 10.096L0 5.5L4.596 0.903999ZM19 8V10H10V8H19ZM19 0.999999V3H10V0.999999H19Z"
          fill="currentColor" />
      </svg>
    </symbol>
    <symbol id="svg-close">
      <svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path
          d="M8 6.2225L14.2225 0L16 1.7775L9.7775 8L16 14.2225L14.2225 16L8 9.7775L1.7775 16L0 14.2225L6.2225 8L0 1.7775L1.7775 0L8 6.2225Z"
          fill="currentColor" />
      </svg>
    </symbol>
    <symbol id="svg-arrow-left">
      <svg width="10" height="17" viewBox="0 0 10 17" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M3.6359 8.5L10 15.1114L8.18205 17L5.82128e-07 8.5L8.18205 -7.9465e-08L10 1.88859L3.6359 8.5Z"
          fill="currentColor" />
      </svg>
    </symbol>
  </svg>
  <div id="global_nav"></div>
  <div class="Container">
    <aside id="sidebar" class="Sidebar" aria-label="Sidebar">
    <div class="Sidebar__animationBg"></div>
    <div class="Sidebar__container">
        <div class="Sidebar__inner">
            <a class="Sidebar__logo" href="../index.html">
                
                <h1>
                    <img class="Sidebar__logoImage--light" src="../_static/logo.png" />
                    <img class="Sidebar__logoImage--dark" src="../_static/logo-dark.png" />
                </h1>
                
            </a>
            <div class="Sidebar__tree Tree"><ul class="current">
<li class="toctree-l1 Tree__item"><a class="reference internal" href="../about.html">About</a></li>
<li class="toctree-l1 Tree__item Tree__item--has-children"><a class="reference internal" href="../projects/index.html">Projects</a><input class="Tree_itemToggleCheckbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="Tree__itemToggle" for="toctree-checkbox-1"><i class="Tree__itemToggleIcon"><svg><use href="#svg-arrow"></use></svg></i></label><ul>
<li class="toctree-l2 Tree__item Tree__item--has-children"><a class="reference internal" href="../projects/ReazonSpeech/index.html">ReazonSpeech</a><input class="Tree_itemToggleCheckbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="Tree__itemToggle" for="toctree-checkbox-2"><i class="Tree__itemToggleIcon"><svg><use href="#svg-arrow"></use></svg></i></label><ul>
<li class="toctree-l3 Tree__item"><a class="reference internal" href="../projects/ReazonSpeech/quickstart.html">クイックスタート</a></li>
<li class="toctree-l3 Tree__item"><a class="reference internal" href="../projects/ReazonSpeech/howto.html">HowToガイド</a></li>
<li class="toctree-l3 Tree__item Tree__item--has-children"><a class="reference internal" href="../projects/ReazonSpeech/api/index.html">APIリファレンス</a><input class="Tree_itemToggleCheckbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="Tree__itemToggle" for="toctree-checkbox-3"><i class="Tree__itemToggleIcon"><svg><use href="#svg-arrow"></use></svg></i></label><ul>
<li class="toctree-l4 Tree__item"><a class="reference internal" href="../projects/ReazonSpeech/api/reazonspeech.nemo.asr.html">reazonspeech.nemo.asr</a></li>
<li class="toctree-l4 Tree__item"><a class="reference internal" href="../projects/ReazonSpeech/api/reazonspeech.k2.asr.html">reazonspeech.k2.asr</a></li>
<li class="toctree-l4 Tree__item"><a class="reference internal" href="../projects/ReazonSpeech/api/reazonspeech.espnet.asr.html">reazonspeech.espnet.asr</a></li>
<li class="toctree-l4 Tree__item"><a class="reference internal" href="../projects/ReazonSpeech/api/reazonspeech.espnet.oneseg.html">reazonspeech.espnet.oneseg</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 Tree__item Tree__item--has-children"><a class="reference internal" href="../news/index.html">News</a><input class="Tree_itemToggleCheckbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="Tree__itemToggle" for="toctree-checkbox-4"><i class="Tree__itemToggleIcon"><svg><use href="#svg-arrow"></use></svg></i></label><ul>
<li class="toctree-l2 Tree__item"><a class="reference internal" href="../news/reazonspeech.html">超高精度で商用利用可能な純国産の日本語音声認識モデル「ReazonSpeech」を無償公開</a></li>
</ul>
</li>
<li class="toctree-l1 current Tree__item Tree__item--has-children"><a class="reference internal" href="index.html">Blog</a><input checked="" class="Tree_itemToggleCheckbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="Tree__itemToggle" for="toctree-checkbox-5"><i class="Tree__itemToggleIcon"><svg><use href="#svg-arrow"></use></svg></i></label><ul class="current">
<li class="toctree-l2 Tree__item"><a class="reference internal" href="2024-08-01-ReazonSpeech.html">(2024-08-01) ReazonSpeech v2.1: Setting a New Standard in Japanese ASR</a></li>
<li class="toctree-l2 current Tree__item current-page"><a class="current reference internal" href="#">(2024-03-02) ALOHAとACTを用いた模倣学習実験の再現方法</a></li>
<li class="toctree-l2 Tree__item"><a class="reference internal" href="2024-02-14-ReazonSpeech.html">(2024-02-14) ReazonSpeech v2.0: 音声モデルの高速化とコーパスの大幅な拡大</a></li>
<li class="toctree-l2 Tree__item"><a class="reference internal" href="2023-04-04-ReazonSpeech.html">(2023-04-04) ReazonSpeechの最新モデルを公開しました</a></li>
<li class="toctree-l2 Tree__item"><a class="reference internal" href="2023-01-15-ReazonSpeech-ESP32.html">(2023-01-15) スマホの通話内容をReazonSpeechで音声認識してSlackに転送する</a></li>
<li class="toctree-l2 Tree__item"><a class="reference internal" href="2023-01-15-DDS-performance.html">(2023-01-15) 分散ROS2 FastDDS/CycloneDDS のパフォーマンス評価</a></li>
</ul>
</li>
<li class="toctree-l1 Tree__item"><a class="reference internal" href="../publications.html">Publications</a></li>
<li class="toctree-l1 Tree__item"><a class="reference internal" href="../careers.html">Careers</a></li>
</ul>
</div>
            <div class="Sidebar__search Search">
                <i class="Search__icon">
                    <svg width="22" height="21" viewBox="0 0 21 21" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <path
                            d="M16.031 14.617L20.314 18.899L18.899 20.314L14.617 16.031C13.0237 17.3082 11.042 18.0029 9 18C4.032 18 0 13.968 0 9C0 4.032 4.032 0 9 0C13.968 0 18 4.032 18 9C18.0029 11.042 17.3082 13.0237 16.031 14.617ZM14.025 13.875C15.2941 12.5699 16.0029 10.8204 16 9C16 5.132 12.867 2 9 2C5.132 2 2 5.132 2 9C2 12.867 5.132 16 9 16C10.8204 16.0029 12.5699 15.2941 13.875 14.025L14.025 13.875Z"
                            fill="currentColor" />
                    </svg>
                </i>
                <form class="Search__form" method="get" action="../search.html" role="search">
                    <input class="Search__formText" placeholder=" Search" name="q" aria-label=" Search" />
                    <input type="hidden" name="check_keywords" value="yes" />
                    <input type="hidden" name="area" value="default" />
                </form>
            </div>
        </div>
    </div>
</aside>
    <div class="Container__inner">
      
      <script>document.body.dataset.theme = localStorage.getItem("theme") || "auto";</script>



<main class="Page">
    <header class="Page__header">
        
        <div class="Page__headerParent">Blog：</div>
        
        <h1>(2024-03-02) ALOHAとACTを用いた模倣学習実験の再現方法 </h1>
        
    </header>
    <article class="Page__body">
        <section id="alohaact">
<h1>(2024-03-02) ALOHAとACTを用いた模倣学習実験の再現方法<a class="headerlink" href="#alohaact" title="この見出しへのパーマリンク">¶</a></h1>
<section id="id1">
<h2>はじめに<a class="headerlink" href="#id1" title="この見出しへのパーマリンク">¶</a></h2>
<p>本記事では、RSS 2023で発表された論文「Learning Fine-Grained Bimanual
Manipulation with Low-Cost
Hardware」で紹介された、低コストのオープンソースハードウェア「ALOHA」を用いて、ACT（Action
Chunking with
Transformers）による模倣学習実験を行うまでの一連の作業について紹介します。</p>
<p>ALOHAとACTの論文:
<a href="https://arxiv.org/abs/2304.13705" target="_blank"
rel="noreferrer noopener">https://arxiv.org/abs/2304.13705</a></p>
<p>ALOHAのGitHubリポジトリ:</p>
<p><a class="reference external" href="https://github.com/tonyzhaozh/aloha">https://github.com/tonyzhaozh/aloha</a></p>
<p>ACTのGitHubリポジトリ:</p>
<p><a class="reference external" href="https://github.com/tonyzhaozh/act">https://github.com/tonyzhaozh/act</a></p>
<p>ALOHAのセットアップについては、GitHubリポジトリに詳しい手順が記載されています。しかし、これらの手順は作業が多岐にわたりやや複雑で時間がかかります。また、ROSのNoetic環境が必要ですが、この記事の執筆時点で多くの方がUbuntu
22.04
LTS以上を使用しているだろうということを踏まえ、ホストPCのOSがNoeticに非対応のバージョンでも容易にセットアップできるようにDockerを利用した環境構築方法を作成しました。これにより、より手軽にALOHAの環境を整えることが可能です。</p>
<p>この記事は、私がレアゾン・ヒューマンインタラクション研究所（Reazon Human
Interaction
Laboratory）が主導するプロジェクトに参加するなかで得た知見に基づいています。<br />
ここでは、プロジェクトでの取り組みを公開し、その成果を共有することを目的としています。
この情報は、Reazon Human Interaction
LaboratoryのブログページとAGIRobotsのブログページの両方で同時に公開されています。<br />
<strong>Reazon Human Interaction Laboratory</strong>:
<a href="https://research.reazon.jp/index.html" target="_blank"
rel="noreferrer noopener">https://research.reazon.jp/index.html</a><br />
<strong>AGIRobots</strong>:
<a href="https://developers.agirobots.com/jp/how-to-run-aloha/"
target="_blank"
rel="noreferrer noopener">https://developers.agirobots.com/jp/how-to-run-aloha/</a></p>
</section>
<section id="id2">
<h2>事前準備<a class="headerlink" href="#id2" title="この見出しへのパーマリンク">¶</a></h2>
<section id="pc">
<h3>要求されるPCの仕様<a class="headerlink" href="#pc" title="この見出しへのパーマリンク">¶</a></h3>
<p>ALOHAでは、4台のアームロボット（ViperX 300とWidowX
250それぞれ2台ずつ）と4台のカメラ（Logitech
C922X）を使用します。これらの機器を同時に使用すると大量のUSB帯域を必要とするため、USB
3.0以上に対応したポートを2つ以上備えたPCが必要です。また、機械学習を行う場合は、<strong>GPUを搭載したPC</strong>、あるいは<strong>外付けGPUの接続が可能なPC</strong>を準備することをお勧めします。</p>
</section>
<section id="os">
<h3>要求されるOS<a class="headerlink" href="#os" title="この見出しへのパーマリンク">¶</a></h3>
<p>この記事で紹介する動作環境の整備ではDockerを使用しますので、Dockerが動作するOSが必要です。また、DockerコンテナからUSBデバイスへのアクセスが可能である必要がありますので、<strong>基本的にはUbuntuを推奨</strong>します。<br />
※Dockerのインストール方法については本記事では紹介していませんので、ご自身でインストールをお願いします。</p>
</section>
</section>
<section id="id3">
<h2>システム概要<a class="headerlink" href="#id3" title="この見出しへのパーマリンク">¶</a></h2>
<p>本記事で構築するALOHAの動作環境の全体的な構成について説明します。以下の図を見てください。</p>
<blockquote>
<div><p><img alt="" src="../_images/image-13-1024x644.png" /></p>
</div></blockquote>
<p>ALOHAは、左右のリーダーとフォロワーのアーム、カメラがそれぞれ2つのUSBハブを通じてPCに接続されます。これらのUSBハブは、PCのUSB
3.0以上のポートに接続されます（帯域が小さいと通信エラーになる可能性あり）。</p>
<p>デバイスをPCに接続すると、ロボットはttyUSB*、カメラはvideo*として認識されます（*は任意の数字）が、この番号は接続の順番によって変化し、PCを再起動するたびに変化してしまう可能性があります。これでは不便ですので、/etc/udev/rules.dにデバイスIDから直接分かりやすい名前として認識するためのルールを記載し、各ロボットアームやカメラが毎回同じ名前で認識されるようにします。認識後の名前は上図の点線の輪で囲まれたようにします（次の章で説明）。</p>
<p>これが完了したら、本記事に記載のDockerfileをビルドし、Dockerイメージを作成します。作成されたDockerイメージからコンテナを作成すると、それはVNCを通じて操作可能で、ディスプレイがないマシンでもネットワークを介して別のPCからアクセスできます。同一マシン上からアクセスする場合は、ブラウザでhttp:127.0.0.1:6080にアクセスしましょう。</p>
<p>また、ACTで模倣学習するには事前に大量のエキスパートデータを用意しておく必要があります。そのエキスパートデータはホストPCに作成した~/datasetsフォルダに保存することで、新しくコンテナを作成しなおしても収集したデータを再利用できるようにします。</p>
<p>※本記事で紹介するDockerfileはAMD64アーキテクチャ向けです。Raspberry
PiやJetsonなどで運用したい場合は、Dockerfileの内容を変更する必要があります。現時点では、これらのデバイス用のDockerfileは作成していませんが、将来的に追加する可能性があります。</p>
<p>以上が、本記事で構築するシステム概要となります。</p>
</section>
<section id="id4">
<h2>デバイス認識名の設定<a class="headerlink" href="#id4" title="この見出しへのパーマリンク">¶</a></h2>
<p>ここでは、接続したデバイス名を、これから設定するファイル（<strong>/etc/udev/rules.d/99-fixed-interbotix-udev.rules</strong>）に記載の通りの名前として認識されるように設定を行います。今回接続するデバイスは、４台のアームロボットと４台のWebカメラです。それぞれについて、シリアル番号を確認する作業が、ここで一番の面倒なところです。</p>
<section id="id5">
<h3>４台のアームロボットの認識名設定<a class="headerlink" href="#id5" title="この見出しへのパーマリンク">¶</a></h3>
<p>USBで接続されたアームロボットは、ttyUSB*の形で認識されますが、*の部分は接続順によって変わります。これを解決するために、デバイスのシリアル番号を用いて、それぞれのアームロボットに固有の名前を割り当てます。</p>
<p><strong>各アームロボットのシリアル番号の取得</strong></p>
<p>まず、シリアル番号を調べるアームロボットのみをPCに接続します。そして、以下のコマンドを実行して、シリアル番号（例:
FT89FIZE）を取得します。<br />
※以下のコマンドでは、ttyUSB0としていますが、アームロボット以外のデバイスが接続されていると、必ずしもttyUSB0であるとは限りませんので、ロボットが接続されているデバイスがどれかを確認し、ttyUSBの番号を適宜変更したうえで実行しましょう。これを4台すべてのアームロボットについて実施します。シリアル番号は忘れないように、メモしておきましょう。</p>
<div class="highlight-prism notranslate"><div class="highlight"><pre><span></span>udevadm info --name=/dev/ttyUSB0 --attribute-walk | grep serial
</pre></div>
</div>
<p><strong>udevルールの編集</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">/etc/udev/rules.d/99-fixed-interbotix-udev.rules</span></code>
を管理者権限のもとエディタで開き、以下のルールを追加します。</p>
<p>左側のリーダーアーム: <code class="docutils literal notranslate"><span class="pre">ttyDXL_master_left</span></code><br />
右側のリーダーアーム: <code class="docutils literal notranslate"><span class="pre">ttyDXL_master_right</span></code><br />
左側のフォロワーアーム: <code class="docutils literal notranslate"><span class="pre">ttyDXL_puppet_left</span></code><br />
右側のフォロワーアーム: <code class="docutils literal notranslate"><span class="pre">ttyDXL_puppet_right</span></code></p>
<p>例えば、左側のリーダアーム（ttyDXL_master_left）のシリアル番号がFT89FIZEのときは、以下のようにルールを記載します。<br />
これを残りのアームロボットについても実施します。その際は、ATTRS{serial}とSYMLINKの部分を適切に書き換えてください。4台のアームロボットについて設定が終わると4行分になります。</p>
<div class="highlight-prism notranslate"><div class="highlight"><pre><span></span>SUBSYSTEM==&quot;tty&quot;, ATTRS{serial}==&quot;FT89FIZE&quot;, ENV{ID_MM_DEVICE_IGNORE}=&quot;1&quot;, ATTR{device/latency_timer}=&quot;1&quot;, SYMLINK+=&quot;ttyDXL_master_left&quot;
</pre></div>
</div>
<p><strong>設定の確認</strong></p>
<p>99-fixed-interbotix-udev.rulesにルールを記載し終えたら、以下のコードを実行します。</p>
<div class="highlight-prism notranslate"><div class="highlight"><pre><span></span>sudo udevadm control --reload &amp;&amp; sudo udevadm trigger
</pre></div>
</div>
<p>そして、4台のアームロボットを接続し、指定した名前で認識されていることを確認して下さい。</p>
</section>
<section id="web">
<h3>4台のWebカメラの認識名設定<a class="headerlink" href="#web" title="この見出しへのパーマリンク">¶</a></h3>
<p>USBで接続されたWebカメラは、video*の形で認識されますが、*の部分は接続順によって変わります。これを解決するために、デバイスのシリアル番号を用いて、それぞれのWebカメラに固有の名前を割り当てます。</p>
<p><strong>各Webカメラのシリアル番号の取得</strong></p>
<p>まず、シリアル番号を知りたいWebカメラのみをPCに接続します。そして、以下のコマンドを実行して、シリアル番号を取得します。<br />
※以下のコマンドでは、video0としていますが、別のカメラが認識されていると、必ずしもvideo0であるとは限りませんので、カメラが接続されているデバイスがどれかを確認したうえで実行しましょう。</p>
<div class="highlight-prism notranslate"><div class="highlight"><pre><span></span>udevadm info --name=/dev/video0 --attribute-walk | grep serial
</pre></div>
</div>
<blockquote>
<div><p>[!NOTE]
上記コマンドを実行しても、認識されない場合があります。</p>
<p>Webカメラが正しく接続されているにもかかわらず、<code class="docutils literal notranslate"><span class="pre">Unknown</span> <span class="pre">device</span> <span class="pre">&quot;/dev/video0&quot;:</span> <span class="pre">No</span> <span class="pre">such</span> <span class="pre">file</span> <span class="pre">or</span> <span class="pre">directory</span></code>が表示されていたら、正しく認識されていません。その場合は、<code class="docutils literal notranslate"><span class="pre">lsusb</span> <span class="pre">-vt</span></code>を実行し、ドライバーが読み込まれているかを確認してください。</p>
<p>ALOHA付属のLogitech C922 Pro Stream
WebcamのVideoのドライバー部分が、<code class="docutils literal notranslate"><span class="pre">Driver=,</span></code>と空白の場合、ドライバーうまく読み込まれていません。このWebカメラはUVCタイプなので、Linuxであればデフォルトで搭載されている<code class="docutils literal notranslate"><span class="pre">uvcvideo</span></code>で問題なく利用できるはずです。</p>
<p><code class="docutils literal notranslate"><span class="pre">dmesg</span></code>コマンドを用いて、デバイスの接続におけるログを確認しましょう。直近20個のログを確認する場合は<code class="docutils literal notranslate"><span class="pre">dmesg</span> <span class="pre">|</span> <span class="pre">tail</span> <span class="pre">-n</span> <span class="pre">20</span></code>を使用します。もし、C922の接続で、<code class="docutils literal notranslate"><span class="pre">Lockdown:</span> <span class="pre">systemd-udevd:</span> <span class="pre">unsigned</span> <span class="pre">module</span> <span class="pre">loading</span> <span class="pre">is</span> <span class="pre">restricted;</span> <span class="pre">see</span> <span class="pre">man</span> <span class="pre">kernel_lockdown.7</span></code>というようなものが記載されていたら、ロックダウン機能が有効化されていることが原因です。これは、セキュアブートが有効化されていると、自動的に設定されてしまうことがあるので、<code class="docutils literal notranslate"><span class="pre">mokutil</span> <span class="pre">--sb-state</span></code>コマンドで確認し、もしセキュアブートが有効化されていたら、BIOS/UEFIを開いて、無効化してください。</p>
</div></blockquote>
<p><strong>udevルールの編集</strong></p>
<p>アームロボットのデバイス名のルールを記載したファイルと同じ<code class="docutils literal notranslate"><span class="pre">/etc/udev/rules.d/99-fixed-interbotix-udev.rules</span></code>
をエディタで開き、以下のルールを追加します。</p>
<p>左側のフォロワーアームのカメラ: <code class="docutils literal notranslate"><span class="pre">CAM_LEFT_WRIST</span></code><br />
右側のフォロワーアームのカメラ: <code class="docutils literal notranslate"><span class="pre">CAM_RIGHT_WRIST</span></code><br />
真ん中付近下のカメラ: <code class="docutils literal notranslate"><span class="pre">CAM_LOW</span></code><br />
真ん中付近上のカメラ: <code class="docutils literal notranslate"><span class="pre">CAM_HIGH</span></code></p>
<p>ルールの例は以下の通りです。ATTRS{serial}とSYMLINKの部分を適切に書き換えてください。4台のカメラを接続するので、4行分のルールを追記してください。</p>
<div class="highlight-prism notranslate"><div class="highlight"><pre><span></span>SUBSYSTEM==&quot;video4linux&quot;, ATTRS{serial}==&quot;&lt;カメラのシリアル番号&gt;&quot;, ATTR{index}==&quot;0&quot;, ATTRS{idProduct}==&quot;085c&quot;, ATTR{device/latency_timer}=&quot;1&quot;, SYMLINK+=&quot;CAM_LEFT_WRIST&quot;
</pre></div>
</div>
<p><strong>設定の確認</strong></p>
<p>99-fixed-interbotix-udev.rulesにルールを記載し終えたら、以下のコードを実行します。</p>
<div class="highlight-prism notranslate"><div class="highlight"><pre><span></span>sudo udevadm control --reload &amp;&amp; sudo udevadm trigger
</pre></div>
</div>
<p>そして、4台のWebカメラを接続し、指定した名前で認識されていることを確認して下さい。</p>
</section>
</section>
<section id="id6">
<h2>グリッパーにトルク制限を設定<a class="headerlink" href="#id6" title="この見出しへのパーマリンク">¶</a></h2>
<p>Dynamixel
Wizardを使用し、フォロワー側のアームのIDが9番のサーボに電流制限を追加し、グリッパが過度に強く閉じて壊れることや、過負荷エラーを防げるようにします。具体的にはアドレス38のCurrent
Limitを300に設定します。</p>
<blockquote>
<div><p><img alt="" src="../_images/image-11-1024x768.png" /></p>
</div></blockquote>
</section>
<section id="docker">
<h2>Dockerイメージのビルド<a class="headerlink" href="#docker" title="この見出しへのパーマリンク">¶</a></h2>
<p>この章では、ALOHAの制御およびACTによる模倣学習を行うためのDockerイメージを作成します。</p>
<p>DockerイメージをビルドするにはDockerfileが必要です。aloha_dockerというフォルダをホームディレクトリ直下に作成し、その中にDockerfileを作成することにします。</p>
<div class="highlight-prism notranslate"><div class="highlight"><pre><span></span>mkdir ~/aloha_docker
</pre></div>
</div>
<p>aloha_dockerフォルダの中で、Dockerfileを作成します。</p>
<div class="highlight-prism notranslate"><div class="highlight"><pre><span></span>nano ~/aloha_docker/Dockerfile
</pre></div>
</div>
<p>Dockerfileには下記のコードを記載します。</p>
<div class="highlight-prism notranslate"><div class="highlight"><pre><span></span>FROM tiryoh/ros-desktop-vnc:noetic

# ROOTにNoeticの環境をインストールする
RUN /tmp/ros_setup_scripts_ubuntu/ros-noetic-desktop.sh

# 必要な依存関係のインストール
RUN apt-get update &amp;&amp; apt-get install -y \
    curl \
    git \
    python3-pip \
    ros-noetic-usb-cam \
    ros-noetic-cv-bridge \
    sudo \
    vim \
    expect \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

# キーボード設定を事前に設定
RUN echo &#39;keyboard-configuration keyboard-configuration/layout select English (US)&#39; | debconf-set-selections
RUN echo &#39;keyboard-configuration keyboard-configuration/layoutcode select &quot;us&quot;&#39; | debconf-set-selections

# XSArmの制御用の各種機能をダウンロード＆インストール
RUN curl &#39;https://raw.githubusercontent.com/Interbotix/interbotix_ros_manipulators/main/interbotix_ros_xsarms/install/amd64/xsarm_amd64_install.sh&#39; &gt; xsarm_amd64_install.sh &amp;&amp; \
    chmod +x xsarm_amd64_install.sh &amp;&amp; \
    ./xsarm_amd64_install.sh -d noetic -n -p /root/interbotix_ws

# ALOHAのリポジトリをクローン
RUN cd /root/interbotix_ws/src &amp;&amp; git clone https://github.com/tonyzhaozh/aloha.git

# ワークスペースのビルド
RUN /bin/bash -c &#39;. /opt/ros/noetic/setup.sh; /root/interbotix_ws/devel/setup.sh; cd /root/interbotix_ws; catkin_make&#39;

# interbotix_xs_toolboxのarm.pyファイルを編集
RUN sed -i &#39;/self.T_sb = mr.FKinSpace(self.robot_des.M, self.robot_des.Slist, self.joint_commands)/c\        self.T_sb = None&#39; /root/interbotix_ws/src/interbotix_ros_toolboxes/interbotix_xs_toolbox/interbotix_xs_modules/src/interbotix_xs_modules/arm.py

# ROS環境とinterbotix_wsのsetup.shをsourceするためのコマンドを.bashrcに追加
RUN echo &quot;source /opt/ros/noetic/setup.sh&quot; &gt;&gt; /root/.bashrc &amp;&amp; \
    echo &quot;source /root/interbotix_ws/devel/setup.sh&quot; &gt;&gt; /root/.bashrc

ENV USER root

RUN mkdir /datasets  # エピソードのデータを保存するためのフォルダ。ホストPC上のディレクトリをマウントする用
# sedコマンドを使用してDATA_DIRを上で作成した/datasetsに変更
RUN sed -i &quot;s|DATA_DIR = &#39;&lt;put your data dir here&gt;&#39;|DATA_DIR = &#39;/datasets&#39;|&quot; &quot;/root/interbotix_ws/src/aloha/aloha_scripts/constants.py&quot;

RUN git clone https://github.com/tonyzhaozh/act.git  # ACT



# Pythonパッケージのインストール
RUN pip3 install torch torchvision pyquaternion pyyaml rospkg pexpect mujoco dm_control opencv-python matplotlib einops packaging h5py ipython
</pre></div>
</div>
<p>このDockerfileをビルドする際には、以下のコードを使用しましょう。以下のコードでは、作成されるイメージにros-noetic-vnc-alohaというタグを付与しています。</p>
<div class="highlight-prism notranslate"><div class="highlight"><pre><span></span>docker build -t ros-noetic-vnc-aloha ~/aloha_docker
</pre></div>
</div>
</section>
<section id="id7">
<h2>Dockerコンテナの起動<a class="headerlink" href="#id7" title="この見出しへのパーマリンク">¶</a></h2>
<p>先ほどビルドしたDockerイメージからコンテナを作成します。これは、docker
runコマンドで行います。その際に、ルートディレクトリに~/aloha/datasetsフォルダを作成し、コンテナにマウントします。先ほど軽く触れましたが、収集したエキスパートデータは全てこの~/aloha/datasetsフォルダの中に保存されます。まだこのフォルダを作成していないと思いますので、以下のコマンドを実行して、ホストPCに~/aloha/datasetsフォルダを作成してください。</p>
<div class="highlight-prism notranslate"><div class="highlight"><pre><span></span>mkdir -p ~/aloha/datasets
</pre></div>
</div>
<p>以下のコマンドを実行して、コンテナを起動しましょう。</p>
<div class="highlight-prism notranslate"><div class="highlight"><pre><span></span>docker run -it -p 6080:80 --privileged --gpus all --volume ~/aloha/datasets:/datasets --volume /dev:/dev --shm-size=512m ros-noetic-vnc-aloha
</pre></div>
</div>
<p>ブラウザからhttp://127.0.0.1:6080にアクセスし、無事、下図のようなリモートデスクトップが起動すれば問題ありません。</p>
<p><img alt="" src="../_images/image-12-1024x640.png" /></p>
</section>
<section id="id8">
<h2>リーダーフォロワーによる操作実験<a class="headerlink" href="#id8" title="この見出しへのパーマリンク">¶</a></h2>
<p>では、まずはリーダーフォロワーによる操作を試してみましょう。ALOHAがPCに接続されていることを確認したうえで、以下の図のように3つのターミナルを開いて下さい。</p>
<p><img alt="" src="../_images/Screenshot-from-2024-01-28-20-06-21-1024x640.png" /></p>
<p>そして、それぞれのターミナルに以下を入力してください。</p>
<div class="highlight-prism notranslate"><div class="highlight"><pre><span></span># 1つ目のターミナル
source /opt/ros/noetic/setup.sh &amp;&amp; source ~/interbotix_ws/devel/setup.sh
roslaunch aloha 4arms_teleop.launch

# 2つ目のターミナル
cd ~/interbotix_ws/src/aloha/aloha_scripts
python3 one_side_teleop.py right

# 3つ目のターミナル
cd ~/interbotix_ws/src/aloha/aloha_scripts
python3 one_side_teleop.py left
</pre></div>
</div>
<p>すると、リーダーアームとフォロワーアームが自動で持ち上がります。リーダーアームのグリッパーを閉じると操作を開始できます。</p>
<p>以下のXに投稿された動画は、テレオペレーションを実施している様子です。</p>
<blockquote>
<div><p>レアゾンHI研では、音声情報や画像情報に加えてロボット制御情報を含めたマルチモーダルデータを的確に処理できる基盤モデルを研究開発し、<br />
高度かつ自然なヒューマンインタラクション技術の実現を目指しています。<br />
このテーマを一緒に探求する研究員やパートナー企業を募集しています!<br />
DM開放してます <a class="reference external" href="https://t.co/TZUn1GIBdR">pic.twitter.com/TZUn1GIBdR</a></p>
<p>— REAZON ヒューマンインタラクション研究所 (&#64;REAZON_HI_Lab) <a class="reference external" href="https://twitter.com/REAZON_HI_Lab/status/1758031208578179580?ref_src=twsrc%5Etfw">February
15,
2024</a></p>
</div></blockquote>
<p>※本来、ALOHAの実験機は、リーダーアームの各関節を上部からゴムで吊るすのですが、ここでは便宜上、上部から吊るしていません。</p>
</section>
<section id="id9">
<h2>エキスパートデータの収集方法<a class="headerlink" href="#id9" title="この見出しへのパーマリンク">¶</a></h2>
<p>リーダーフォロワーによる制御が確認できたら、次はACTで模倣学習をするための準備として、エキスパートデータを収集してみましょう。</p>
<section id="constants-py">
<h3>constants.pyにタスクの種類を記載<a class="headerlink" href="#constants-py" title="この見出しへのパーマリンク">¶</a></h3>
<p>エキスパートデータを収集する前に、これから収集するデータのタスク内容について、alohaリポジトリの中にあるconstants.pyに追記する必要があります。デフォルトだと以下のように、aloha_wear_shoeのみが記載されています。</p>
<div class="highlight-wp-block-code notranslate"><div class="highlight"><pre><span></span>TASK_CONFIGS = {
    &#39;aloha_wear_shoe&#39;:{
        &#39;dataset_dir&#39;: DATA_DIR + &#39;/aloha_wear_shoe&#39;,
        &#39;num_episodes&#39;: 50,
        &#39;episode_len&#39;: 1000,
        &#39;camera_names&#39;: [&#39;cam_high&#39;, &#39;cam_low&#39;, &#39;cam_left_wrist&#39;, &#39;cam_right_wrist&#39;]
    },
}
</pre></div>
</div>
<p>ここに、収集したいデータのタスクについての情報を追加します。（あまり良い例ではないですが）例えば、動作テスト（operation_test）なら、以下のようにして既存のタスクセットに追加することが考えられます。</p>
<div class="highlight-wp-block-code notranslate"><div class="highlight"><pre><span></span>TASK_CONFIGS = {
    &#39;aloha_wear_shoe&#39;:{
        &#39;dataset_dir&#39;: DATA_DIR + &#39;/aloha_wear_shoe&#39;,
        &#39;num_episodes&#39;: 50,
        &#39;episode_len&#39;: 1000,
        &#39;camera_names&#39;: [&#39;cam_high&#39;, &#39;cam_low&#39;, &#39;cam_left_wrist&#39;, &#39;cam_right_wrist&#39;]
    },
    &#39;operation_test&#39;:{
        &#39;dataset_dir&#39;: DATA_DIR + &#39;/operation_test&#39;,
        &#39;num_episodes&#39;: 50,
        &#39;episode_len&#39;: 1000,
        &#39;camera_names&#39;: [&#39;cam_high&#39;, &#39;cam_low&#39;, &#39;cam_left_wrist&#39;, &#39;cam_right_wrist&#39;]
    },
}
</pre></div>
</div>
</section>
<section id="id10">
<h3>エキスパートデータの収集<a class="headerlink" href="#id10" title="この見出しへのパーマリンク">¶</a></h3>
<p>データの収集はrecord_episoeds.pyを実行することで行います。その際には、タスクの名前と、エピソードの番号を指定します。以下は、先程constants.pyに定義したoperation_testタスクにおけるエピソード0番目の収集を行う場合の例です。</p>
<div class="highlight-prism notranslate"><div class="highlight"><pre><span></span>python3 record_episodes.py --task_name operation_test --episode_idx 0
</pre></div>
</div>
</section>
<section id="id11">
<h3>収集したエピソードデータからWebカメラの映像を生成<a class="headerlink" href="#id11" title="この見出しへのパーマリンク">¶</a></h3>
<p>Webカメラから収集された映像が学習に適しているかを確認してみましょう。引数のdataset_dirは省略できないのがちょっと面倒ですが、以下のようなコマンドを実行してください。</p>
<div class="highlight-prism notranslate"><div class="highlight"><pre><span></span>python3 visualize_episodes.py --dataset_dir /datasets/operation_test --episode_idx 0
</pre></div>
</div>
<p>これを実行すると、このエピソードの実行時に撮影された４つのカメラからの映像が１つの映像にまとめられてファイルとして出力されます。左から順番に、トップビュー、左側アームからのビュー、ボトムビュー、右側アームからのビューです。</p>
<p>一番左側の映像に注目していただきたいのですが、アームが映っていません。つまり、Webカメラの角度が適切でないことが確認できます。このような場合は、<strong>上に設置したWebカメラの角度を調節し、フォロワーのアームロボットが正しく映るように</strong>してください。</p>
</section>
<section id="id12">
<h3>動作の再生<a class="headerlink" href="#id12" title="この見出しへのパーマリンク">¶</a></h3>
<p>以下を実行すると、収集した動作が実機で再生されます。</p>
<div class="highlight-prism notranslate"><div class="highlight"><pre><span></span>python3 replay_episodes.py --dataset_dir /datasets/operation_test --episode_idx 0
</pre></div>
</div>
</section>
</section>
<section id="act">
<h2>ACTによる模倣学習<a class="headerlink" href="#act" title="この見出しへのパーマリンク">¶</a></h2>
<p>コンテナ内でターミナルを開き、<code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code>を実行し、GPUが使用できるか確認しましょう。問題なく実行できるのであれば、以下のようにGPUの情報が表示されます。</p>
<blockquote>
<div><p><img alt="" src="../_images/image-6-1024x621.png" /></p>
</div></blockquote>
<section id="id13">
<h3>十分なエキスパートデータの収集<a class="headerlink" href="#id13" title="この見出しへのパーマリンク">¶</a></h3>
<p>先ほど、エキスパートデータの収集方法について記載しました。ACTで模倣学習し、高い精度をで動作を獲得するには、それなりに沢山のエキスパートデータが必要になりますので、適宜収集してください。</p>
<p>本記事では、エラーが無く学習できるところまでを確認することが目的ですので、箱からサーボを取り出す動作を10エピソードだけ収集してみました。</p>
</section>
<section id="id14">
<h3>模倣学習の実行<a class="headerlink" href="#id14" title="この見出しへのパーマリンク">¶</a></h3>
<p>模倣学習をするには以下のコードを実行します。</p>
<p>&lt;ckpt dir&gt;には学習済みモデルを保存するディレクトリを指定します。</p>
<div class="highlight-prism notranslate"><div class="highlight"><pre><span></span>cd ~/act
python3 imitate_episodes.py \
--task_name operation_test \
--ckpt_dir &lt;ckpt dir&gt; \
--policy_class ACT --kl_weight 10 --chunk_size 100 --hidden_dim 512 --batch_size 8 --dim_feedforward 3200 \
--num_epochs 2000  --lr 1e-5 \
--seed 0
</pre></div>
</div>
</section>
<section id="id15">
<h3>学習後のモデルによる実機の動作<a class="headerlink" href="#id15" title="この見出しへのパーマリンク">¶</a></h3>
<p>エラーが無く学習を終了できたら、実機で動作を再現してみましょう。</p>
<p>再現するときは、先ほど模倣学習を行った時に実行したimitate_episodes.pyの引数に、--evalを追加するだけです。</p>
<div class="highlight-prism notranslate"><div class="highlight"><pre><span></span>cd ~/act
python3 imitate_episodes.py \
--task_name operation_test \
--ckpt_dir &lt;ckpt dir&gt; \
--policy_class ACT --kl_weight 10 --chunk_size 100 --hidden_dim 512 --batch_size 8 --dim_feedforward 3200 \
--num_epochs 2000  --lr 1e-5 \
--seed 0 \
--eval
</pre></div>
</div>
<p>収集したエキスパートデータの質が高くなく、量が少ないので、動作は変な感じですね（笑）。それと、アームが激しく振動しているのでカメラが揺れてます（笑）。</p>
<p><a class="reference external" href="https://www.youtube.com/watch?v=OopJM3L8no8">www.youtube.com</a></p>
<p>今後、別のタスクで、膨大なエキスパートデータを収集し、模倣学習実験を行う予定なので、良い動作が実現できたら追記したいと思います。</p>
</section>
</section>
<section id="id16">
<h2>さいごに<a class="headerlink" href="#id16" title="この見出しへのパーマリンク">¶</a></h2>
<p>本記事では、ALOHAとACTを用いた模倣学習実験の再現方法について説明しました。エラー無く最後まで実行できましたでしょうか？</p>
<p>本記事執筆時点では、まだ動作の再現ができただけで、技術の本質的な部分にふれられていないので、より深い部分にたどり着けるよう、実験を行っていきたいと思います。</p>
<p>最後までお読みいただきありがとうございました。</p>
<p>Copyright © 2024 <a class="reference external" href="https://developers.agirobots.com/jp">AGIRobots Blog</a>
All Rights Reserved.</p>
</section>
</section>

    </article>
    <aside class="Page__toc Toc">
        
        
        <div class="Toc__inner">
            <div class="Toc__title">
                <span>Index</span>
            </div>
            <div class="Toc__tree">
                <ul>
<li><a class="reference internal" href="#">(2024-03-02) ALOHAとACTを用いた模倣学習実験の再現方法</a><ul>
<li><a class="reference internal" href="#id1">はじめに</a></li>
<li><a class="reference internal" href="#id2">事前準備</a><ul>
<li><a class="reference internal" href="#pc">要求されるPCの仕様</a></li>
<li><a class="reference internal" href="#os">要求されるOS</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id3">システム概要</a></li>
<li><a class="reference internal" href="#id4">デバイス認識名の設定</a><ul>
<li><a class="reference internal" href="#id5">４台のアームロボットの認識名設定</a></li>
<li><a class="reference internal" href="#web">4台のWebカメラの認識名設定</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id6">グリッパーにトルク制限を設定</a></li>
<li><a class="reference internal" href="#docker">Dockerイメージのビルド</a></li>
<li><a class="reference internal" href="#id7">Dockerコンテナの起動</a></li>
<li><a class="reference internal" href="#id8">リーダーフォロワーによる操作実験</a></li>
<li><a class="reference internal" href="#id9">エキスパートデータの収集方法</a><ul>
<li><a class="reference internal" href="#constants-py">constants.pyにタスクの種類を記載</a></li>
<li><a class="reference internal" href="#id10">エキスパートデータの収集</a></li>
<li><a class="reference internal" href="#id11">収集したエピソードデータからWebカメラの映像を生成</a></li>
<li><a class="reference internal" href="#id12">動作の再生</a></li>
</ul>
</li>
<li><a class="reference internal" href="#act">ACTによる模倣学習</a><ul>
<li><a class="reference internal" href="#id13">十分なエキスパートデータの収集</a></li>
<li><a class="reference internal" href="#id14">模倣学習の実行</a></li>
<li><a class="reference internal" href="#id15">学習後のモデルによる実機の動作</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id16">さいごに</a></li>
</ul>
</li>
</ul>

            </div>
        </div>
        
        
    </aside>
</main>

    </div>
  </div>
  <footer class="Footer">
    <div class="Footer__copyright">
        Copyright &copy; 2023, Human Interaction Laboratory
    </div>
    <div class="Footer__toolButton ToolButton">
        <div class="ToolButton__viewMode">
            <div class="ToolButton__viewModeLabel">View Mode</div>
            <button class="ToolButton__viewModeButton ToolButton__viewModeButton--dark">Night Mode</button>
            <button class="ToolButton__viewModeButton ToolButton__viewModeButton--light">Day Mode</button>
            <button class="ToolButton__viewModeButton ToolButton__viewModeButton--system">System Setting</button>
        </div>
<!--
        <div class="ToolButton__languageMode">
            <div class="ToolButton__languageModeLabel">Language</div>
            <div class="ToolButton__languageModeButtons">
                <a class="ToolButton__languageModeButton" disabled
                    href="../../blog/2024-03-02-how-to-run-aloha-developers.agirobots.com.html">JP</a>
                /
                <a class="ToolButton__languageModeButton"
                    href=".././en/blog/2024-03-02-how-to-run-aloha-developers.agirobots.com.html">EN</a>
            </div>
        </div>
-->
    </div>
</footer><script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
  <script src="../_static/jquery.js"></script>
  <script src="../_static/underscore.js"></script>
  <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
  <script src="../_static/doctools.js"></script>
  <script src="../_static/sphinx_highlight.js"></script>
  <script src="../_static/translations.js"></script>
  <!-- <script type="module" crossorigin src="http://localhost:3000/javascripts/main.ts"></script> -->
  <script src="../_static/main.js" defer></script></body>

</html>