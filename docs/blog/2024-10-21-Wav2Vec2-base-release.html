<!doctype html>
<html class="no-js"  lang="ja" >

<head><meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-S1KMDX1V1H"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-S1KMDX1V1H');
    </script>
    
  <link rel="author" title="このドキュメントについて" href="../about.html" /><link rel="index" title="索引" href="../genindex.html" /><link rel="search" title="検索" href="../search.html" /><link rel="next" title="(2024-08-01) ReazonSpeech v2.1: Setting a New Standard in Japanese ASR" href="2024-08-01-ReazonSpeech.html" /><link rel="prev" title="(2024-11-06) 第1回 OpenArm勉強会を開催しました!" href="2024-11-06-openarm-study-group-01.html" />
  <link rel="canonical" href="https://research.reazon.jp/blog/2024-10-21-Wav2Vec2-base-release.html" />
  
  <title>(2024-10-21) 大規模日本語音声による事前学習モデルWav2Vec2を公開 - Reazon Human Interaction Lab</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Roboto+Condensed:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
  <link rel="stylesheet" type="text/css" href="../_static/style.css" />
  <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
  </head>

<body id="blog-2024-10-21-Wav2Vec2-base-release">
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
    <symbol id="svg-arrow" viewBox="0 0 14 8">
      <svg viewBox="0 0 14 8" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7 2.82801L2.05 7.77801L0.635998 6.36401L7 1.47024e-05L13.364 6.36402L11.95 7.77802L7 2.82801Z"
          fill="currentColor" />
      </svg>
    </symbol>
    <symbol id="svg-toc-menu">
      <svg width="19" height="17" viewBox="0 0 19 17" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path
          d="M19 15V17H1V15H19ZM4.596 0.903999L6.01 2.318L2.828 5.5L6.01 8.682L4.596 10.096L0 5.5L4.596 0.903999ZM19 8V10H10V8H19ZM19 0.999999V3H10V0.999999H19Z"
          fill="currentColor" />
      </svg>
    </symbol>
    <symbol id="svg-close">
      <svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path
          d="M8 6.2225L14.2225 0L16 1.7775L9.7775 8L16 14.2225L14.2225 16L8 9.7775L1.7775 16L0 14.2225L6.2225 8L0 1.7775L1.7775 0L8 6.2225Z"
          fill="currentColor" />
      </svg>
    </symbol>
    <symbol id="svg-arrow-left">
      <svg width="10" height="17" viewBox="0 0 10 17" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M3.6359 8.5L10 15.1114L8.18205 17L5.82128e-07 8.5L8.18205 -7.9465e-08L10 1.88859L3.6359 8.5Z"
          fill="currentColor" />
      </svg>
    </symbol>
  </svg>
  <div id="global_nav"></div>
  <div class="Container">
    <aside id="sidebar" class="Sidebar" aria-label="Sidebar">
    <div class="Sidebar__animationBg"></div>
    <div class="Sidebar__container">
        <div class="Sidebar__inner">
            <a class="Sidebar__logo" href="../index.html">
                
                <h1>
                    <img class="Sidebar__logoImage--light" src="../_static/logo.png" />
                    <img class="Sidebar__logoImage--dark" src="../_static/logo-dark.png" />
                </h1>
                
            </a>
            <div class="Sidebar__tree Tree"><ul class="current">
<li class="toctree-l1 Tree__item"><a class="reference internal" href="../about.html">About</a></li>
<li class="toctree-l1 Tree__item Tree__item--has-children"><a class="reference internal" href="../projects/index.html">Projects</a><input class="Tree_itemToggleCheckbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="Tree__itemToggle" for="toctree-checkbox-1"><i class="Tree__itemToggleIcon"><svg><use href="#svg-arrow"></use></svg></i></label><ul>
<li class="toctree-l2 Tree__item Tree__item--has-children"><a class="reference internal" href="../projects/ReazonSpeech/index.html">ReazonSpeech</a><input class="Tree_itemToggleCheckbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="Tree__itemToggle" for="toctree-checkbox-2"><i class="Tree__itemToggleIcon"><svg><use href="#svg-arrow"></use></svg></i></label><ul>
<li class="toctree-l3 Tree__item"><a class="reference internal" href="../projects/ReazonSpeech/quickstart.html">クイックスタート</a></li>
<li class="toctree-l3 Tree__item"><a class="reference internal" href="../projects/ReazonSpeech/howto.html">HowToガイド</a></li>
<li class="toctree-l3 Tree__item Tree__item--has-children"><a class="reference internal" href="../projects/ReazonSpeech/api/index.html">APIリファレンス</a><input class="Tree_itemToggleCheckbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="Tree__itemToggle" for="toctree-checkbox-3"><i class="Tree__itemToggleIcon"><svg><use href="#svg-arrow"></use></svg></i></label><ul>
<li class="toctree-l4 Tree__item"><a class="reference internal" href="../projects/ReazonSpeech/api/reazonspeech.nemo.asr.html">reazonspeech.nemo.asr</a></li>
<li class="toctree-l4 Tree__item"><a class="reference internal" href="../projects/ReazonSpeech/api/reazonspeech.k2.asr.html">reazonspeech.k2.asr</a></li>
<li class="toctree-l4 Tree__item"><a class="reference internal" href="../projects/ReazonSpeech/api/reazonspeech.espnet.asr.html">reazonspeech.espnet.asr</a></li>
<li class="toctree-l4 Tree__item"><a class="reference internal" href="../projects/ReazonSpeech/api/reazonspeech.espnet.oneseg.html">reazonspeech.espnet.oneseg</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 Tree__item Tree__item--has-children"><a class="reference internal" href="../news/index.html">News</a><input class="Tree_itemToggleCheckbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="Tree__itemToggle" for="toctree-checkbox-4"><i class="Tree__itemToggleIcon"><svg><use href="#svg-arrow"></use></svg></i></label><ul>
<li class="toctree-l2 Tree__item"><a class="reference internal" href="../news/reazonspeech.html">超高精度で商用利用可能な純国産の日本語音声認識モデル「ReazonSpeech」を無償公開</a></li>
</ul>
</li>
<li class="toctree-l1 current Tree__item Tree__item--has-children"><a class="reference internal" href="index.html">Blog</a><input checked="" class="Tree_itemToggleCheckbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="Tree__itemToggle" for="toctree-checkbox-5"><i class="Tree__itemToggleIcon"><svg><use href="#svg-arrow"></use></svg></i></label><ul class="current">
<li class="toctree-l2 Tree__item"><a class="reference internal" href="2024-11-06-openarm-study-group-01.html">(2024-11-06) 第1回 OpenArm勉強会を開催しました!</a></li>
<li class="toctree-l2 current Tree__item current-page"><a class="current reference internal" href="#">(2024-10-21) 大規模日本語音声による事前学習モデルWav2Vec2を公開</a></li>
<li class="toctree-l2 Tree__item"><a class="reference internal" href="2024-08-01-ReazonSpeech.html">(2024-08-01) ReazonSpeech v2.1: Setting a New Standard in Japanese ASR</a></li>
<li class="toctree-l2 Tree__item"><a class="reference internal" href="2024-03-02-how-to-run-aloha-developers.agirobots.com.html">(2024-03-02) ALOHAとACTを用いた模倣学習実験の再現方法</a></li>
<li class="toctree-l2 Tree__item"><a class="reference internal" href="2024-02-14-ReazonSpeech.html">(2024-02-14) ReazonSpeech v2.0: 音声モデルの高速化とコーパスの大幅な拡大</a></li>
<li class="toctree-l2 Tree__item"><a class="reference internal" href="2023-04-04-ReazonSpeech.html">(2023-04-04) ReazonSpeechの最新モデルを公開しました</a></li>
<li class="toctree-l2 Tree__item"><a class="reference internal" href="2023-01-15-ReazonSpeech-ESP32.html">(2023-01-15) スマホの通話内容をReazonSpeechで音声認識してSlackに転送する</a></li>
<li class="toctree-l2 Tree__item"><a class="reference internal" href="2023-01-15-DDS-performance.html">(2023-01-15) 分散ROS2 FastDDS/CycloneDDS のパフォーマンス評価</a></li>
</ul>
</li>
<li class="toctree-l1 Tree__item"><a class="reference internal" href="../publications.html">Publications</a></li>
<li class="toctree-l1 Tree__item"><a class="reference internal" href="../careers.html">Careers</a></li>
</ul>
</div>
            <div class="Sidebar__search Search">
                <i class="Search__icon">
                    <svg width="22" height="21" viewBox="0 0 21 21" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <path
                            d="M16.031 14.617L20.314 18.899L18.899 20.314L14.617 16.031C13.0237 17.3082 11.042 18.0029 9 18C4.032 18 0 13.968 0 9C0 4.032 4.032 0 9 0C13.968 0 18 4.032 18 9C18.0029 11.042 17.3082 13.0237 16.031 14.617ZM14.025 13.875C15.2941 12.5699 16.0029 10.8204 16 9C16 5.132 12.867 2 9 2C5.132 2 2 5.132 2 9C2 12.867 5.132 16 9 16C10.8204 16.0029 12.5699 15.2941 13.875 14.025L14.025 13.875Z"
                            fill="currentColor" />
                    </svg>
                </i>
                <form class="Search__form" method="get" action="../search.html" role="search">
                    <input class="Search__formText" placeholder=" Search" name="q" aria-label=" Search" />
                    <input type="hidden" name="check_keywords" value="yes" />
                    <input type="hidden" name="area" value="default" />
                </form>
            </div>
        </div>
    </div>
</aside>
    <div class="Container__inner">
      
      <script>document.body.dataset.theme = localStorage.getItem("theme") || "auto";</script>



<main class="Page">
    <header class="Page__header">
        
        <div class="Page__headerParent">Blog：</div>
        
        <h1>(2024-10-21) 大規模日本語音声による事前学習モデルWav2Vec2を公開 </h1>
        
    </header>
    <article class="Page__body">
        <section id="wav2vec2">
<h1>(2024-10-21) 大規模日本語音声による事前学習モデルWav2Vec2を公開<a class="headerlink" href="#wav2vec2" title="この見出しへのパーマリンク">¶</a></h1>
<p>35,000時間の大規模日本語音声コーパスReazonSpeech v2.0を用いて事前学習及びファインチューニングを行ったWav2Vec2 [1]を公開しました！</p>
<p>本記事では、公開したモデルのベンチマーク結果と、Transformersライブラリを用いたWav2Vec2の事前学習の方法を紹介します。</p>
<p>今回公開したモデルは、以下の2つのWav2Vec2 Baseモデルです。</p>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/reazon-research/japanese-wav2vec2-base"><code class="docutils literal notranslate"><span class="pre">reazon-research/japanese-wav2vec2-base</span></code></a></p>
<ul>
<li><p>ReazonSpeech v2.0コーパスを用いて事前学習を行ったモデル</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://huggingface.co/reazon-research/japanese-wav2vec2-base-rs35kh"><code class="docutils literal notranslate"><span class="pre">reazon-research/japanese-wav2vec2-base-rs35kh</span></code></a></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">reazon-research/japanese-wav2vec2-base</span></code>をReazonSpeech v2.0コーパスを用いてCTCファインチューニングを行ったモデル</p></li>
</ul>
</li>
</ul>
<section id="id1">
<h2>ベンチマーク結果<a class="headerlink" href="#id1" title="この見出しへのパーマリンク">¶</a></h2>
<p>ここでは、CTCファインチューニングを行った<code class="docutils literal notranslate"><span class="pre">reazon-research/japanese-wav2vec2-base-rs35kh</span></code>とその他Wav2Vec2ファミリーのCTCモデルを用いて、日本語音声の書き起こし性能のベンチマークを行います。<br />
<code class="docutils literal notranslate"><span class="pre">japanese-wav2vec2-base-rs35kh</span></code>の学習データとして用いていない、JSUT-BASIC5000 [2]、Common Voice [3]、TEDxJP-10K [4]の3つのデータセットを用いて性能を検証します。
評価指標には、CER (Character Error Rate)を用います。</p>
<p>ベンチマークに使用したモデルと学習データは以下の表の通りです。
モデルは全てHugging Face上に公開されています。</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>モデル名</p></th>
<th class="head text-left"><p>事前学習モデル</p></th>
<th class="head text-center"><p>学習データ</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">reazon-research/japanese-wav2vec2-base-rs35kh</span></code></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">reazon-research/japanese-wav2vec2-base</span></code></p></td>
<td class="text-center"><p>ReazonSpeech v2.0</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">Ivydata/wav2vec2-large-xlsr-53-japanese</span></code></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">facebook/wav2vec2-large-xlsr-53</span></code></p></td>
<td class="text-center"><p>Common Voice, JVS, JSUT</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">jonatasgrosman/wav2vec2-large-xlsr-53-japanese</span></code></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">facebook/wav2vec2-large-xlsr-53</span></code></p></td>
<td class="text-center"><p>Common Voice, CSS10, JSUT</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">vumichien/wav2vec2-large-xlsr-japanese</span></code></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">facebook/wav2vec2-large-xlsr-53</span></code></p></td>
<td class="text-center"><p>Common Voice, JSUT</p></td>
</tr>
</tbody>
</table>
</div>
<p>※ XLSR [5]とは、53言語の音声データを用いて、Wav2Vec2.0の学習フレームワークにより事前学習を行ったモデルです。</p>
<p><img alt="benchmark" src="../_images/bench.png" /></p>
<p>上図はベンチマーク結果を示しており、AVERAGEは3つのデータセットでのCERの平均値です。</p>
<p>結果から、<code class="docutils literal notranslate"><span class="pre">japanese-wav2vec2-base-rs35kh</span></code>はAVERAGEで最もCERが低く、良い性能を示しています。</p>
<p>しかし、JSUT-BASIC5000やCommon Voiceでは、他モデルの方が非常に良い性能を示す場合があります。
他モデルは学習データにJSUT-BASIC5000やCommon Voiceを用いているため、テストデータとして用いたデータが学習データに含まれている可能性が考えられます。</p>
<p>ベンチマークに使用した全てのテストデータは<code class="docutils literal notranslate"><span class="pre">japanese-wav2vec2-base-rs35kh</span></code>の学習データに含まれていないが、AVERAGEでは最も良い性能を示しており、十分な性能が得られているのではないでしょうか。</p>
<p><img alt="jsut-book" src="../_images/jsut-book.png" /></p>
<p>試しに長時間音声が含まれるJSUT-Bookコーパスでベンチマークを行うと、上図のような結果になりました。</p>
<p>長時間音声に対しては、<code class="docutils literal notranslate"><span class="pre">japanese-wav2vec2-base-rs35kh</span></code>はもっとも悪い性能となっています。</p>
<p>学習に用いたReazonSpeech v2.0コーパスには、数分単位の長時間音声が含まれないため、長時間音声を一気に処理するには向いていないようです。</p>
<p>長時間音声に対応するためには、短い音声セグメントに区切ってから処理したり、長時間音声用にファインチューニングするなどの工夫が必要そうです。</p>
<details>
<summary>評価コード参考</summary>
<p>以下のコードは推論とCERの計算を行うサンプルコードです。</p>
<p>※ <code class="docutils literal notranslate"><span class="pre">vumichien/wav2vec2-large-xlsr-japanese</span></code>の推論結果には、トークン間に空白が入ってしまうため、
モデルの推論結果から空白を除くよう処理を加えました。<br />
(例: 今 流行 の 単身 赴任 族 の 淋し さ を ちょっぴり 味ごば せ て もらっ た の も 有 意義 の 体験 だ)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">num2words</span>
<span class="kn">import</span> <span class="nn">editdistance</span>
<span class="kn">import</span> <span class="nn">librosa</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoProcessor</span><span class="p">,</span> <span class="n">Wav2Vec2ForCTC</span>

<span class="n">PUNCTUATIONS</span> <span class="o">=</span> <span class="p">{</span><span class="nb">ord</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="s2">&quot;&quot;</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="s2">&quot;、。「」『』，,？！!!?!?&quot;</span><span class="p">}</span>
<span class="n">ZENKAKU</span> <span class="o">=</span> <span class="s2">&quot;ａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺ０１２３４５６７８９&quot;</span>
<span class="n">HANKAKU</span> <span class="o">=</span> <span class="s2">&quot;abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789&quot;</span>
<span class="n">ZEN2HAN</span> <span class="o">=</span> <span class="nb">str</span><span class="o">.</span><span class="n">maketrans</span><span class="p">(</span><span class="n">ZENKAKU</span><span class="p">,</span> <span class="n">HANKAKU</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">normalize</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">translate</span><span class="p">(</span><span class="n">PUNCTUATIONS</span><span class="p">)</span><span class="o">.</span><span class="n">translate</span><span class="p">(</span><span class="n">ZEN2HAN</span><span class="p">)</span>
    <span class="n">conv</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">m</span><span class="p">:</span> <span class="n">num2words</span><span class="o">.</span><span class="n">num2words</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;ja&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\d+\.?\d*&#39;</span><span class="p">,</span> <span class="n">conv</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">Wav2Vec2ForCTC</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>

<span class="n">audio</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">librosa</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">audio_filepath</span><span class="p">)</span>
<span class="n">audio</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="n">pad_width</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="mi">16_000</span><span class="p">))</span>
<span class="n">input_values</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span>
    <span class="n">audio</span><span class="p">,</span>
    <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
    <span class="n">sampling_rate</span><span class="o">=</span><span class="mi">16_000</span>
<span class="p">)</span><span class="o">.</span><span class="n">input_values</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_values</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
<span class="n">predicted_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">transcription</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">predicted_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">asr</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="n">transcription</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">))</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="n">dist</span> <span class="o">=</span> <span class="n">editdistance</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">asr</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
<span class="n">cer</span> <span class="o">=</span> <span class="n">dist</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
</details>
</section>
<section id="wav2vec2transformers">
<h2>Wav2Vec2をTransformersライブラリ基盤で事前学習するには？<a class="headerlink" href="#wav2vec2transformers" title="この見出しへのパーマリンク">¶</a></h2>
<p>Wav2Vec2をTransformersライブラリ基盤の上で事前学習するには、少しだけコツが必要です。</p>
<p>まず、<code class="docutils literal notranslate"><span class="pre">Transformers.Trainer</span></code>を用いて学習するためには、モデルが持つ<code class="docutils literal notranslate"><span class="pre">forward</span></code>メソッドの引数に<code class="docutils literal notranslate"><span class="pre">labels</span></code>、出力に<code class="docutils literal notranslate"><span class="pre">loss</span></code>を持つ必要があります。<br />
<strong>たとえ、<code class="docutils literal notranslate"><span class="pre">labels</span></code>を与える必要がなくても、必須です。</strong><br />
Transformersに用意されているWav2Vec2の事前学習用モデルクラスの<code class="docutils literal notranslate"><span class="pre">Wav2Vec2ForPreTraining</span></code>には、<code class="docutils literal notranslate"><span class="pre">labels</span></code>を引数を取ることができません。<br />
Wav2Vec2は自己教師あり学習フレームワークに基づいて学習されており、<strong>外部から正解ラベルを与える必要がないため</strong>、このクラスの設計になっているのだと思います。</p>
<p>そのため、以下のように、ダミーとして<code class="docutils literal notranslate"><span class="pre">labels</span></code>を引数に取ることができるモデルクラスでラップすることで、<code class="docutils literal notranslate"><span class="pre">Transformers.Trainer</span></code>を用いて簡単に学習することができます。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Wav2Vec2ForPreTraining</span>


<span class="k">class</span> <span class="nc">Wav2Vec2ForPreTrainingWrapper</span><span class="p">(</span><span class="n">Wav2Vec2ForPreTraining</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_values</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">mask_time_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">sampled_negative_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># labelsをダミーで設定</span>
    <span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
            <span class="n">input_values</span><span class="o">=</span><span class="n">input_values</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
            <span class="n">mask_time_indices</span><span class="o">=</span><span class="n">mask_time_indices</span><span class="p">,</span>
            <span class="n">sampled_negative_indices</span><span class="o">=</span><span class="n">sampled_negative_indices</span><span class="p">,</span>
        <span class="p">)</span>  <span class="c1"># もちろんlabelsは使うことはない</span>
        <span class="k">return</span> <span class="n">outputs</span>
</pre></div>
</div>
</section>
<section id="id2">
<h2>おわりに<a class="headerlink" href="#id2" title="この見出しへのパーマリンク">¶</a></h2>
<p>今回、35,000時間の大規模日本語音声コーパスReazonSpeech v2.0を用いて事前学習およびファインチューニングを行ったWav2Vec2を公開しました。</p>
<p>Apache 2.0ライセンスにて公開するため、ぜひ学術研究等で広くご活用ください！</p>
<p>また、ファインチューニングデータを揃えたベンチマークなどによる詳細なモデル評価や、別データで継続学習を行った派生モデルの開発などのフィードバックも、広くお待ちしております！</p>
<hr class="docutils" />
<p>[1] Baevski, A., Zhou, Y., Mohamed, A. and Auli, M., 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33, pp.12449-12460.<br />
[2] <a class="reference external" href="https://sites.google.com/site/shinnosuketakamichi/publication/jsut">https://sites.google.com/site/shinnosuketakamichi/publication/jsut</a><br />
[3] <a class="reference external" href="https://commonvoice.mozilla.org/">https://commonvoice.mozilla.org/</a><br />
[4] <a class="reference external" href="https://github.com/laboroai/TEDxJP-10K">https://github.com/laboroai/TEDxJP-10K</a><br />
[5] Conneau, A., Baevski, A., Collobert, R., Mohamed, A. and Auli, M., 2020. Unsupervised cross-lingual representation learning for speech recognition. arXiv preprint arXiv:2006.13979.</p>
</section>
</section>

    </article>
    <aside class="Page__toc Toc">
        
        
        <div class="Toc__inner">
            <div class="Toc__title">
                <span>Index</span>
            </div>
            <div class="Toc__tree">
                <ul>
<li><a class="reference internal" href="#">(2024-10-21) 大規模日本語音声による事前学習モデルWav2Vec2を公開</a><ul>
<li><a class="reference internal" href="#id1">ベンチマーク結果</a></li>
<li><a class="reference internal" href="#wav2vec2transformers">Wav2Vec2をTransformersライブラリ基盤で事前学習するには？</a></li>
<li><a class="reference internal" href="#id2">おわりに</a></li>
</ul>
</li>
</ul>

            </div>
        </div>
        
        
    </aside>
</main>

    </div>
  </div>
  <footer class="Footer">
    <div class="Footer__copyright">
        Copyright &copy; 2023, Human Interaction Laboratory
    </div>
    <div class="Footer__toolButton ToolButton">
        <div class="ToolButton__viewMode">
            <div class="ToolButton__viewModeLabel">View Mode</div>
            <button class="ToolButton__viewModeButton ToolButton__viewModeButton--dark">Night Mode</button>
            <button class="ToolButton__viewModeButton ToolButton__viewModeButton--light">Day Mode</button>
            <button class="ToolButton__viewModeButton ToolButton__viewModeButton--system">System Setting</button>
        </div>
<!--
        <div class="ToolButton__languageMode">
            <div class="ToolButton__languageModeLabel">Language</div>
            <div class="ToolButton__languageModeButtons">
                <a class="ToolButton__languageModeButton" disabled
                    href="../../blog/2024-10-21-Wav2Vec2-base-release.html">JP</a>
                /
                <a class="ToolButton__languageModeButton"
                    href=".././en/blog/2024-10-21-Wav2Vec2-base-release.html">EN</a>
            </div>
        </div>
-->
    </div>
</footer><script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
  <script src="../_static/jquery.js"></script>
  <script src="../_static/underscore.js"></script>
  <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
  <script src="../_static/doctools.js"></script>
  <script src="../_static/sphinx_highlight.js"></script>
  <script src="../_static/translations.js"></script>
  <!-- <script type="module" crossorigin src="http://localhost:3000/javascripts/main.ts"></script> -->
  <script src="../_static/main.js" defer></script></body>

</html>